import tensorflow as tf
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import json
import os

def log(message):
    log_file_path = "python_log.txt"
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(message + "\n")

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª–∞–º (–∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è Unity)
current_path = os.path.abspath(__file__)
while not current_path.endswith("Assets"):
    current_path = os.path.dirname(current_path)

unity_assets_path = os.path.join(current_path, "AIModels")
model_path = os.path.join(unity_assets_path, "text_model.keras")
dataset_path = os.path.join(current_path, "Dataset", "Dataset.json")

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
log(f"Dataset path: {dataset_path}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
if not os.path.exists(dataset_path):
    log(f"[ERROR] Dataset not found at: {dataset_path}")
    raise FileNotFoundError(f"Dataset not found at: {dataset_path}")

with open(dataset_path, "r", encoding="utf-8") as file:
    data = json.load(file)

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
log(f"[INFO] Dataset loaded. Sample data: {data[:2]}")  # –ü–µ—á–∞—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2 –∑–∞–ø–∏—Å–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
X = np.array(
    [[d["SentimentScore"], d["ManipulativeWordRatio"], d["LexicalDiversity"], d["SubjectivityScore"]] for d in data])
y_labels = [d["Label"] for d in data]
y_conclusions = [d["Conclusion"] for d in data]

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
log(f"[INFO] Normalizing data...")  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
label_encoder = LabelEncoder()
y_labels_encoded = label_encoder.fit_transform(y_labels)  # –¢–µ–ø–µ—Ä—å —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞

conclusion_encoder = LabelEncoder()
y_conclusions_encoded = conclusion_encoder.fit_transform(y_conclusions)  # –¢–µ–ø–µ—Ä—å —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
if os.path.exists(model_path):
    model = tf.keras.models.load_model(model_path)
    log(f"Model loaded from: {model_path}")
else:
    input_layer = tf.keras.layers.Input(shape=(4,))
    hidden_layer = tf.keras.layers.Dense(8, activation="relu")(input_layer)

    label_output = tf.keras.layers.Dense(len(set(y_labels)), activation="softmax", name="label_output")(hidden_layer)
    conclusion_output = tf.keras.layers.Dense(len(set(y_conclusions)), activation="softmax", name="conclusion_output")(
        hidden_layer)

    model = tf.keras.Model(inputs=input_layer, outputs=[label_output, conclusion_output])
    model.compile(optimizer="adam",
                  loss=["sparse_categorical_crossentropy", "sparse_categorical_crossentropy"],
                  metrics=["accuracy", "accuracy"])

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor="loss", patience=20, restore_best_weights=True
    )

    log(f"[INFO] Training model...")
    model.fit(X, [y_labels_encoded, y_conclusions_encoded], epochs=150, batch_size=8, callbacks=[early_stopping])
    model.save(model_path)
    log(f"Model saved to: {model_path}")


def predict_text(json_file_path):
    try:
        log(f"üìÇ Checking file path: {json_file_path}")

        if not os.path.exists(json_file_path):
            log("‚ùå File does not exist!")
            return "Error", "File not found"

        if os.path.getsize(json_file_path) == 0:
            log("‚ùå File is empty!")
            return "Error", "Empty file"

        with open(json_file_path, "r", encoding="utf-8-sig") as file:
            content = file.read()
            log(f"üìÑ File content:\n{content}")
            data = json.loads(content)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å
        log(f"[INFO] Input data to model: {data}")

        input_data = np.array([[data[key] for key in
                                ["SentimentScore", "ManipulativeWordRatio", "LexicalDiversity", "SubjectivityScore"]]])
        input_data = scaler.transform(input_data)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å
        log(f"[INFO] Normalized input data: {input_data}")

        pred_label, pred_conclusion = model.predict(input_data)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        log(f"[INFO] Model raw prediction: Label={pred_label}, Conclusion={pred_conclusion}")

        label_index = np.argmax(pred_label)
        conclusion_index = np.argmax(pred_conclusion)

        decoded_label = label_encoder.inverse_transform([label_index])[0]
        decoded_conclusion = conclusion_encoder.inverse_transform([conclusion_index])[0]

        log(f"[INFO] Prediction: Label={decoded_label}, Conclusion={decoded_conclusion}")
        return str(decoded_label), str(decoded_conclusion)

    except Exception as e:
        log(f"[ERROR] Python Error: {e}")
        return "Python Error", str(e)
